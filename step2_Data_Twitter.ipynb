{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, urllib, re, json\n",
    "import numba\n",
    "from timeit import default_timer as timer\n",
    "from numba import jit, njit\n",
    "from numba import *\n",
    "import string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import preprocessor as p\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def clean_EMOJI(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    " \n",
    "    #after tweepy preprocessing the colon left remain after removing mentions\n",
    "    #or RT sign in the beginning of the tweet\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Äö√Ñ¬∂', '', tweet)\n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    " \n",
    " \n",
    "    #remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    " \n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    " \n",
    "    #looping through conditions\n",
    "    for w in word_tokens:\n",
    "        #check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)\n",
    "    \n",
    "# @numba.jit(nopython=True, forceobj=True)   \n",
    "def regular_process(text):\n",
    "    text = str(text)\n",
    "#     text = text.lower()\n",
    "    text = re.sub(r\"\\+\", \"\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"!\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \"\", text)\n",
    "    text = re.sub(r\"'\", \"\", text).strip()   \n",
    "    text = re.sub(r\":\", \"\", text).strip()   \n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()   \n",
    "    text = re.sub('RT','',text)\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def remove_URL(sample):\n",
    "    \n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    res = \"\"\n",
    "    for word in sample.split():\n",
    "        if(word.find(\"http\") != -1):\n",
    "            word = \"\"\n",
    "            res = res+word\n",
    "        else:\n",
    "            res = res+\" \"+word\n",
    "        #res.join(\" \")\n",
    "    return res\n",
    "\n",
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words.split():\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words.split():\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words.split():\n",
    "        if(word.find('#')!=-1 or word.find('@')!=-1):\n",
    "            new_words.append(word)\n",
    "        else:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words.split():\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word, andword=\"\")\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def lemmas_words(sentence):\n",
    "\n",
    "    filtered_sent = []\n",
    "    filtered_sent_tag = {}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = sentence.split()\n",
    "#     print(words)\n",
    "    ps = PorterStemmer()\n",
    "#     print(pos_tag(words))\n",
    "    for word, tag in pos_tag(words):\n",
    "        if(word.find('#')!=-1 or word.find('@')!=-1):\n",
    "            filtered_sent.append(word)\n",
    "        else:\n",
    "            wntag= tag[0].lower()\n",
    "            wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "            lemma = lemmatizer.lemmatize(word, wntag) if wntag else word\n",
    "            filtered_sent.append(lemma)\n",
    "        \n",
    "    return \" \".join(filtered_sent)\n",
    "\n",
    "def preprocess_by_stemming(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    text = re.sub('rt',' ',str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = regular_process(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmas_words(words)\n",
    "    return words\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample = remove_URL(sample)\n",
    "    sample = replace_contractions(sample)\n",
    "    return normalize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('tweets_covid19_before_24_April_cleaned.csv')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@henrymakow fyi thirty thousand, three hundred five source show #covid19 corona virus hype likely fraud'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test5= 'RT @HenryMakow: FYI 30305 Sources s showing the #COVID19 / corona virus is over hyped, and likely fraud.\\n\\nhttps://t.co/FeyFqivla9'\n",
    "# print(test5)\n",
    "res = preprocess(test5)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without GPU: 9.827770467964001\n",
      "with GPU: 2.3705030330456793\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from numba import jit, njit\n",
    "# normal function to run on cpu \n",
    "\n",
    "def func(a):\n",
    "    for i in range(10000000): \n",
    "        a[i]+= 1\n",
    "\n",
    "# function optimized to run on gpu \n",
    "# @jit\n",
    "# @cuda.jit\n",
    "# @cuda.jit(device=True)\n",
    "\n",
    "@jit\n",
    "def func2(a): \n",
    "    for i in range(10000000): \n",
    "        a[i]+= 1\n",
    "if __name__==\"__main__\": \n",
    "    n = 10000000\n",
    "    a = np.ones(n, dtype = np.float64) \n",
    "    b = np.ones(n, dtype = np.float32) \n",
    "    \n",
    "    start = timer() \n",
    "    func(a) \n",
    "    print(\"without GPU:\", timer()-start)\n",
    "\n",
    "    start = timer() \n",
    "    func2(a) \n",
    "    print(\"with GPU:\", timer()-start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @numba.jit(nopython=True, forceobj=True)\n",
    "def get_duplicate(AllTweets):\n",
    "    result = pd.DataFrame()\n",
    "    tweetChecklist = {}\n",
    "    for current_tweet in AllTweets:\n",
    "        # If tweet doesn't exist in the list\n",
    "        if current_tweet not in tweetChecklist:\n",
    "            j=0\n",
    "            tweetChecklist[current_tweet]=j;\n",
    "        else:\n",
    "            tweetChecklist[current_tweet]= tweetChecklist[current_tweet]+1\n",
    "#     print()\n",
    "    result.append(tweetChecklist,ignore_index=True)\n",
    "    return tweetChecklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates = get_duplicate(all_tweets_covid19.original_text)\n",
    "# duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_Parkinsons=\"tweets_Parkinsons.csv\"\n",
    "# tweets_Epilepsy=\"tweets_Epilepsy.csv\"\n",
    "# tweets_HeartDisease=\"tweets_HeartDisease.csv\"\n",
    "# #tweets_Parkinson_ = pd.read_csv(tweets_Parkinsons)\n",
    "# #tweets_Epilepsy_ = pd.read_csv(tweets_Epilepsy)\n",
    "# #tweets_HeartDisease_ = pd.read_csv(tweets_HeartDisease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "      <th>place_coord_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1187341321901629441</td>\n",
       "      <td>Thu Oct 24 12:13:25 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @AmEpilepsySoc: Terrific leadership team an...</td>\n",
       "      <td>@amepilepsysoc terrific leadership team great ...</td>\n",
       "      <td>Sentiment(polarity=0.4, subjectivity=0.875)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.875</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>OeGNeurologie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>epilepsy</td>\n",
       "      <td>AmEpilepsySoc</td>\n",
       "      <td>Wien</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1187341273490952193</td>\n",
       "      <td>Thu Oct 24 12:13:13 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @AmEpilepsySoc: Have you used our interacti...</td>\n",
       "      <td>@amepilepsysoc used interactive program plan #...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OeGNeurologie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AES2019</td>\n",
       "      <td>AmEpilepsySoc</td>\n",
       "      <td>Wien</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1187340551567400960</td>\n",
       "      <td>Thu Oct 24 12:10:21 +0000 2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Any epilepsy nurses out there using T.E.C to g...</td>\n",
       "      <td>epilepsy nurses using tec gather data quolie 1...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mammylaine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1187340485616115712</td>\n",
       "      <td>Thu Oct 24 12:10:06 +0000 2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @YellaSunshine: My family suffered a huge l...</td>\n",
       "      <td>@yellasunshine family suffered huge loss passi...</td>\n",
       "      <td>Sentiment(polarity=0.10625000000000004, subjec...</td>\n",
       "      <td>0.10625000000000004</td>\n",
       "      <td>0.7</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Coolesssst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YellaSunshine</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1187340465663815682</td>\n",
       "      <td>Thu Oct 24 12:10:01 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://socialbee.io/\" rel=\"nofollow\"...</td>\n",
       "      <td>RT @aMundaneSarah: Every day can be a struggle...</td>\n",
       "      <td>@amundanesarah every day struggle disability #...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>aMundaneSarah</td>\n",
       "      <td>False</td>\n",
       "      <td>epilepsy, AD</td>\n",
       "      <td>aMundaneSarah</td>\n",
       "      <td>East Midlands, England</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8618</th>\n",
       "      <td>8618</td>\n",
       "      <td>8618</td>\n",
       "      <td>1234121903008288771</td>\n",
       "      <td>Sun Mar 01 14:22:45 +0000 2020</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @LotusOak2: #DidYouKnow that of the U.S. ch...</td>\n",
       "      <td>@lotusoak2 #didyouknow us children experience ...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>xenonian1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>Roma Italy</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8619</th>\n",
       "      <td>8619</td>\n",
       "      <td>8619</td>\n",
       "      <td>1234121012817485824</td>\n",
       "      <td>Sun Mar 01 14:19:12 +0000 2020</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @LotusOak2: #DidYouKnow that of the U.S. ch...</td>\n",
       "      <td>@lotusoak2 #didyouknow us children experience ...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>goldwaterkid65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8620</th>\n",
       "      <td>8620</td>\n",
       "      <td>8620</td>\n",
       "      <td>1234120252600078336</td>\n",
       "      <td>Sun Mar 01 14:16:11 +0000 2020</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @LotusOak2: #DidYouKnow that of the U.S. ch...</td>\n",
       "      <td>@lotusoak2 #didyouknow us children experience ...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>jjulz01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>Tennessee, USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8621</th>\n",
       "      <td>8621</td>\n",
       "      <td>8621</td>\n",
       "      <td>1234119519112716288</td>\n",
       "      <td>Sun Mar 01 14:13:16 +0000 2020</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @LotusOak2: #DidYouKnow that of the U.S. ch...</td>\n",
       "      <td>@lotusoak2 #didyouknow us children experience ...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>qqqueenbeee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>TwilightZoneüê∏</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8622</th>\n",
       "      <td>8622</td>\n",
       "      <td>8622</td>\n",
       "      <td>1234119323632984067</td>\n",
       "      <td>Sun Mar 01 14:12:30 +0000 2020</td>\n",
       "      <td>&lt;a href=\"http://ViraBurnayeva.com\" rel=\"nofoll...</td>\n",
       "      <td>#DidYouKnow that of the U.S. children who expe...</td>\n",
       "      <td>#didyouknow us children experience mmr-vaccine...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>False</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoVA, USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8623 rows √ó 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1                   id  \\\n",
       "0              0             0  1187341321901629441   \n",
       "1              1             1  1187341273490952193   \n",
       "2              2             2  1187340551567400960   \n",
       "3              3             3  1187340485616115712   \n",
       "4              4             4  1187340465663815682   \n",
       "...          ...           ...                  ...   \n",
       "8618        8618          8618  1234121903008288771   \n",
       "8619        8619          8619  1234121012817485824   \n",
       "8620        8620          8620  1234120252600078336   \n",
       "8621        8621          8621  1234119519112716288   \n",
       "8622        8622          8622  1234119323632984067   \n",
       "\n",
       "                          created_at  \\\n",
       "0     Thu Oct 24 12:13:25 +0000 2019   \n",
       "1     Thu Oct 24 12:13:13 +0000 2019   \n",
       "2     Thu Oct 24 12:10:21 +0000 2019   \n",
       "3     Thu Oct 24 12:10:06 +0000 2019   \n",
       "4     Thu Oct 24 12:10:01 +0000 2019   \n",
       "...                              ...   \n",
       "8618  Sun Mar 01 14:22:45 +0000 2020   \n",
       "8619  Sun Mar 01 14:19:12 +0000 2020   \n",
       "8620  Sun Mar 01 14:16:11 +0000 2020   \n",
       "8621  Sun Mar 01 14:13:16 +0000 2020   \n",
       "8622  Sun Mar 01 14:12:30 +0000 2020   \n",
       "\n",
       "                                                 source  \\\n",
       "0     <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "1     <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "2     <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3     <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4     <a href=\"https://socialbee.io/\" rel=\"nofollow\"...   \n",
       "...                                                 ...   \n",
       "8618  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "8619  <a href=\"http://twitter.com/download/android\" ...   \n",
       "8620  <a href=\"http://twitter.com/download/android\" ...   \n",
       "8621  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "8622  <a href=\"http://ViraBurnayeva.com\" rel=\"nofoll...   \n",
       "\n",
       "                                          original_text  \\\n",
       "0     RT @AmEpilepsySoc: Terrific leadership team an...   \n",
       "1     RT @AmEpilepsySoc: Have you used our interacti...   \n",
       "2     Any epilepsy nurses out there using T.E.C to g...   \n",
       "3     RT @YellaSunshine: My family suffered a huge l...   \n",
       "4     RT @aMundaneSarah: Every day can be a struggle...   \n",
       "...                                                 ...   \n",
       "8618  RT @LotusOak2: #DidYouKnow that of the U.S. ch...   \n",
       "8619  RT @LotusOak2: #DidYouKnow that of the U.S. ch...   \n",
       "8620  RT @LotusOak2: #DidYouKnow that of the U.S. ch...   \n",
       "8621  RT @LotusOak2: #DidYouKnow that of the U.S. ch...   \n",
       "8622  #DidYouKnow that of the U.S. children who expe...   \n",
       "\n",
       "                                             clean_text  \\\n",
       "0     @amepilepsysoc terrific leadership team great ...   \n",
       "1     @amepilepsysoc used interactive program plan #...   \n",
       "2     epilepsy nurses using tec gather data quolie 1...   \n",
       "3     @yellasunshine family suffered huge loss passi...   \n",
       "4     @amundanesarah every day struggle disability #...   \n",
       "...                                                 ...   \n",
       "8618  @lotusoak2 #didyouknow us children experience ...   \n",
       "8619  @lotusoak2 #didyouknow us children experience ...   \n",
       "8620  @lotusoak2 #didyouknow us children experience ...   \n",
       "8621  @lotusoak2 #didyouknow us children experience ...   \n",
       "8622  #didyouknow us children experience mmr-vaccine...   \n",
       "\n",
       "                                              sentiment             polarity  \\\n",
       "0           Sentiment(polarity=0.4, subjectivity=0.875)                  0.4   \n",
       "1             Sentiment(polarity=0.0, subjectivity=0.0)                  0.0   \n",
       "2             Sentiment(polarity=0.0, subjectivity=0.0)                  0.0   \n",
       "3     Sentiment(polarity=0.10625000000000004, subjec...  0.10625000000000004   \n",
       "4             Sentiment(polarity=0.0, subjectivity=0.0)                  0.0   \n",
       "...                                                 ...                  ...   \n",
       "8618        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "8619        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "8620        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "8621        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "8622        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "\n",
       "     subjectivity lang favorite_count retweet_count original_author  \\\n",
       "0           0.875   en              0             2   OeGNeurologie   \n",
       "1             0.0   en              0             0   OeGNeurologie   \n",
       "2             0.0   en              0             0      Mammylaine   \n",
       "3             0.7   en              0             2      Coolesssst   \n",
       "4             0.0   en              0             8   aMundaneSarah   \n",
       "...           ...  ...            ...           ...             ...   \n",
       "8618          0.0   en              0            14       xenonian1   \n",
       "8619          0.0   en              0            14  goldwaterkid65   \n",
       "8620          0.0   en              0            14         jjulz01   \n",
       "8621          0.0   en              0            14     qqqueenbeee   \n",
       "8622          0.0   en              6            14       LotusOak2   \n",
       "\n",
       "     possibly_sensitive                        hashtags  user_mentions  \\\n",
       "0                   NaN                        epilepsy  AmEpilepsySoc   \n",
       "1                   NaN                         AES2019  AmEpilepsySoc   \n",
       "2                   NaN                             NaN            NaN   \n",
       "3                   NaN                             NaN  YellaSunshine   \n",
       "4                 False                    epilepsy, AD  aMundaneSarah   \n",
       "...                 ...                             ...            ...   \n",
       "8618                NaN  DidYouKnow, seizures, Epilepsy      LotusOak2   \n",
       "8619                NaN  DidYouKnow, seizures, Epilepsy      LotusOak2   \n",
       "8620                NaN  DidYouKnow, seizures, Epilepsy      LotusOak2   \n",
       "8621                NaN  DidYouKnow, seizures, Epilepsy      LotusOak2   \n",
       "8622              False  DidYouKnow, seizures, Epilepsy            NaN   \n",
       "\n",
       "                       place place_coord_boundaries  \n",
       "0                       Wien                    NaN  \n",
       "1                       Wien                    NaN  \n",
       "2                        NaN                    NaN  \n",
       "3             Washington, DC                    NaN  \n",
       "4     East Midlands, England                    NaN  \n",
       "...                      ...                    ...  \n",
       "8618              Roma Italy                    NaN  \n",
       "8619           United States                    NaN  \n",
       "8620          Tennessee, USA                    NaN  \n",
       "8621           TwilightZoneüê∏                    NaN  \n",
       "8622               NoVA, USA                    NaN  \n",
       "\n",
       "[8623 rows x 19 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_epilepsy_1 = pd.read_csv('raw_Twitter_data/tweets_Parkinsons.csv')\n",
    "tweets_Epilepsy_2 = pd.read_csv('tweets_Epilepsy_1MARS.csv')\n",
    "tweets_epilepsy_1 = tweets_epilepsy_1.drop_duplicates().reset_index(drop=True)\n",
    "tweets_Epilepsy_2= tweets_Epilepsy_2.drop_duplicates().reset_index(drop=True)\n",
    "tweets_Epilepsy_3 = pd.read_csv('tweets_Epilepsy_1MARS.csv2')\n",
    "tweets_Epilepsy_3= tweets_Epilepsy_3.drop_duplicates().reset_index(drop=True)\n",
    "tweets_epilepsy = pd.read_csv('tweets_epilepsy.csv')\n",
    "tweets_epilepsy = tweets_epilepsy.drop_duplicates().reset_index(drop=True)\n",
    "tweets_epilepsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8623"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [tweets_epilepsy_1, tweets_Epilepsy_2, tweets_Epilepsy_3]\n",
    "tweets_epilepsy= pd.concat(frames)\n",
    "tweets_epilepsy = tweets_epilepsy.drop_duplicates().reset_index(drop=True)\n",
    "len(tweets_epilepsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_epilepsy.to_csv('tweets_epilepsy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "Tweets_covid19_after19MARCH = pd.read_csv(\"COVID19/tweets_covid19_after19MARCH.csv\")\n",
    "Tweets_covid19_after19MARCH = Tweets_covid19_after19MARCH.drop_duplicates().reset_index(drop=True)\n",
    "print(len(Tweets_covid19_after19MARCH))\n",
    "sentences = list(map(lambda x: preprocess(x),Tweets_covid19_after19MARCH.original_text))\n",
    "Tweets_covid19_after19MARCH.original_text = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [tweets, Tweets_covid19_after19MARCH]\n",
    "all_tweets_covid19 = pd.concat(frames)\n",
    "all_tweets_covid19 = all_tweets_covid19.drop_duplicates().reset_index(drop=True)\n",
    "all_tweets_covid19.to_csv('all_tweets_covid19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_epilepsy.original_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>source</th>\n",
       "      <th>original_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lang</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_mentions</th>\n",
       "      <th>place</th>\n",
       "      <th>place_coord_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1187341321901629441</td>\n",
       "      <td>Thu Oct 24 12:13:25 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @AmEpilepsySoc: Terrific leadership team an...</td>\n",
       "      <td>RT AmEpilepsySoc Terrific leadership team grea...</td>\n",
       "      <td>Sentiment(polarity=0.4, subjectivity=0.875)</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.875</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>OeGNeurologie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>epilepsy</td>\n",
       "      <td>AmEpilepsySoc</td>\n",
       "      <td>Wien</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1187341273490952193</td>\n",
       "      <td>Thu Oct 24 12:13:13 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @AmEpilepsySoc: Have you used our interacti...</td>\n",
       "      <td>RT AmEpilepsySoc Have used interactive program...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>OeGNeurologie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AES2019</td>\n",
       "      <td>AmEpilepsySoc</td>\n",
       "      <td>Wien</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1187340551567400960</td>\n",
       "      <td>Thu Oct 24 12:10:21 +0000 2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Any epilepsy nurses out there using T.E.C to g...</td>\n",
       "      <td>Any epilepsy nurses using T.E.C gather data QU...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mammylaine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1187340485616115712</td>\n",
       "      <td>Thu Oct 24 12:10:06 +0000 2019</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>RT @YellaSunshine: My family suffered a huge l...</td>\n",
       "      <td>RT YellaSunshine My family suffered huge loss ...</td>\n",
       "      <td>Sentiment(polarity=0.10625000000000004, subjec...</td>\n",
       "      <td>0.10625000000000004</td>\n",
       "      <td>0.7</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Coolesssst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YellaSunshine</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1187340465663815682</td>\n",
       "      <td>Thu Oct 24 12:10:01 +0000 2019</td>\n",
       "      <td>&lt;a href=\"https://socialbee.io/\" rel=\"nofollow\"...</td>\n",
       "      <td>RT @aMundaneSarah: Every day can be a struggle...</td>\n",
       "      <td>RT aMundaneSarah Every day struggle disability...</td>\n",
       "      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>aMundaneSarah</td>\n",
       "      <td>False</td>\n",
       "      <td>epilepsy, AD</td>\n",
       "      <td>aMundaneSarah</td>\n",
       "      <td>East Midlands, England</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8618</td>\n",
       "      <td>8618</td>\n",
       "      <td>1234121903008288771</td>\n",
       "      <td>Sun Mar 01 14:22:45 +0000 2020</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @LotusOak2: #DidYouKnow that of the U.S. ch...</td>\n",
       "      <td>RT LotusOak2 DidYouKnow U.S. children experien...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>xenonian1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>Roma Italy</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8619</td>\n",
       "      <td>8619</td>\n",
       "      <td>1234121012817485824</td>\n",
       "      <td>Sun Mar 01 14:19:12 +0000 2020</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @LotusOak2: #DidYouKnow that of the U.S. ch...</td>\n",
       "      <td>RT LotusOak2 DidYouKnow U.S. children experien...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>goldwaterkid65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8620</td>\n",
       "      <td>8620</td>\n",
       "      <td>1234120252600078336</td>\n",
       "      <td>Sun Mar 01 14:16:11 +0000 2020</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @LotusOak2: #DidYouKnow that of the U.S. ch...</td>\n",
       "      <td>RT LotusOak2 DidYouKnow U.S. children experien...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>jjulz01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>Tennessee, USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8621</td>\n",
       "      <td>8621</td>\n",
       "      <td>1234119519112716288</td>\n",
       "      <td>Sun Mar 01 14:13:16 +0000 2020</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @LotusOak2: #DidYouKnow that of the U.S. ch...</td>\n",
       "      <td>RT LotusOak2 DidYouKnow U.S. children experien...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>qqqueenbeee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>TwilightZoneüê∏</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8622</td>\n",
       "      <td>8622</td>\n",
       "      <td>1234119323632984067</td>\n",
       "      <td>Sun Mar 01 14:12:30 +0000 2020</td>\n",
       "      <td>&lt;a href=\"http://ViraBurnayeva.com\" rel=\"nofoll...</td>\n",
       "      <td>#DidYouKnow that of the U.S. children who expe...</td>\n",
       "      <td>DidYouKnow U.S. children experience MMR-vaccin...</td>\n",
       "      <td>Sentiment(polarity=-0.05, subjectivity=0.0)</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>en</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>LotusOak2</td>\n",
       "      <td>False</td>\n",
       "      <td>DidYouKnow, seizures, Epilepsy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoVA, USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8623 rows √ó 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                   id                      created_at  \\\n",
       "0              0  1187341321901629441  Thu Oct 24 12:13:25 +0000 2019   \n",
       "1              1  1187341273490952193  Thu Oct 24 12:13:13 +0000 2019   \n",
       "2              2  1187340551567400960  Thu Oct 24 12:10:21 +0000 2019   \n",
       "3              3  1187340485616115712  Thu Oct 24 12:10:06 +0000 2019   \n",
       "4              4  1187340465663815682  Thu Oct 24 12:10:01 +0000 2019   \n",
       "...          ...                  ...                             ...   \n",
       "8618        8618  1234121903008288771  Sun Mar 01 14:22:45 +0000 2020   \n",
       "8619        8619  1234121012817485824  Sun Mar 01 14:19:12 +0000 2020   \n",
       "8620        8620  1234120252600078336  Sun Mar 01 14:16:11 +0000 2020   \n",
       "8621        8621  1234119519112716288  Sun Mar 01 14:13:16 +0000 2020   \n",
       "8622        8622  1234119323632984067  Sun Mar 01 14:12:30 +0000 2020   \n",
       "\n",
       "                                                 source  \\\n",
       "0     <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "1     <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "2     <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "3     <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4     <a href=\"https://socialbee.io/\" rel=\"nofollow\"...   \n",
       "...                                                 ...   \n",
       "8618  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "8619  <a href=\"http://twitter.com/download/android\" ...   \n",
       "8620  <a href=\"http://twitter.com/download/android\" ...   \n",
       "8621  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "8622  <a href=\"http://ViraBurnayeva.com\" rel=\"nofoll...   \n",
       "\n",
       "                                          original_text  \\\n",
       "0     RT @AmEpilepsySoc: Terrific leadership team an...   \n",
       "1     RT @AmEpilepsySoc: Have you used our interacti...   \n",
       "2     Any epilepsy nurses out there using T.E.C to g...   \n",
       "3     RT @YellaSunshine: My family suffered a huge l...   \n",
       "4     RT @aMundaneSarah: Every day can be a struggle...   \n",
       "...                                                 ...   \n",
       "8618  RT @LotusOak2: #DidYouKnow that of the U.S. ch...   \n",
       "8619  RT @LotusOak2: #DidYouKnow that of the U.S. ch...   \n",
       "8620  RT @LotusOak2: #DidYouKnow that of the U.S. ch...   \n",
       "8621  RT @LotusOak2: #DidYouKnow that of the U.S. ch...   \n",
       "8622  #DidYouKnow that of the U.S. children who expe...   \n",
       "\n",
       "                                             clean_text  \\\n",
       "0     RT AmEpilepsySoc Terrific leadership team grea...   \n",
       "1     RT AmEpilepsySoc Have used interactive program...   \n",
       "2     Any epilepsy nurses using T.E.C gather data QU...   \n",
       "3     RT YellaSunshine My family suffered huge loss ...   \n",
       "4     RT aMundaneSarah Every day struggle disability...   \n",
       "...                                                 ...   \n",
       "8618  RT LotusOak2 DidYouKnow U.S. children experien...   \n",
       "8619  RT LotusOak2 DidYouKnow U.S. children experien...   \n",
       "8620  RT LotusOak2 DidYouKnow U.S. children experien...   \n",
       "8621  RT LotusOak2 DidYouKnow U.S. children experien...   \n",
       "8622  DidYouKnow U.S. children experience MMR-vaccin...   \n",
       "\n",
       "                                              sentiment             polarity  \\\n",
       "0           Sentiment(polarity=0.4, subjectivity=0.875)                  0.4   \n",
       "1             Sentiment(polarity=0.0, subjectivity=0.0)                  0.0   \n",
       "2             Sentiment(polarity=0.0, subjectivity=0.0)                  0.0   \n",
       "3     Sentiment(polarity=0.10625000000000004, subjec...  0.10625000000000004   \n",
       "4             Sentiment(polarity=0.0, subjectivity=0.0)                  0.0   \n",
       "...                                                 ...                  ...   \n",
       "8618        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "8619        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "8620        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "8621        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "8622        Sentiment(polarity=-0.05, subjectivity=0.0)                -0.05   \n",
       "\n",
       "     subjectivity lang favorite_count retweet_count original_author  \\\n",
       "0           0.875   en              0             2   OeGNeurologie   \n",
       "1             0.0   en              0             0   OeGNeurologie   \n",
       "2             0.0   en              0             0      Mammylaine   \n",
       "3             0.7   en              0             2      Coolesssst   \n",
       "4             0.0   en              0             8   aMundaneSarah   \n",
       "...           ...  ...            ...           ...             ...   \n",
       "8618          0.0   en              0            14       xenonian1   \n",
       "8619          0.0   en              0            14  goldwaterkid65   \n",
       "8620          0.0   en              0            14         jjulz01   \n",
       "8621          0.0   en              0            14     qqqueenbeee   \n",
       "8622          0.0   en              6            14       LotusOak2   \n",
       "\n",
       "     possibly_sensitive                        hashtags  user_mentions  \\\n",
       "0                   NaN                        epilepsy  AmEpilepsySoc   \n",
       "1                   NaN                         AES2019  AmEpilepsySoc   \n",
       "2                   NaN                             NaN            NaN   \n",
       "3                   NaN                             NaN  YellaSunshine   \n",
       "4                 False                    epilepsy, AD  aMundaneSarah   \n",
       "...                 ...                             ...            ...   \n",
       "8618                NaN  DidYouKnow, seizures, Epilepsy      LotusOak2   \n",
       "8619                NaN  DidYouKnow, seizures, Epilepsy      LotusOak2   \n",
       "8620                NaN  DidYouKnow, seizures, Epilepsy      LotusOak2   \n",
       "8621                NaN  DidYouKnow, seizures, Epilepsy      LotusOak2   \n",
       "8622              False  DidYouKnow, seizures, Epilepsy            NaN   \n",
       "\n",
       "                       place place_coord_boundaries  \n",
       "0                       Wien                    NaN  \n",
       "1                       Wien                    NaN  \n",
       "2                        NaN                    NaN  \n",
       "3             Washington, DC                    NaN  \n",
       "4     East Midlands, England                    NaN  \n",
       "...                      ...                    ...  \n",
       "8618              Roma Italy                    NaN  \n",
       "8619           United States                    NaN  \n",
       "8620          Tennessee, USA                    NaN  \n",
       "8621           TwilightZoneüê∏                    NaN  \n",
       "8622               NoVA, USA                    NaN  \n",
       "\n",
       "[8623 rows x 18 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_epilepsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       RT @AmEpilepsySoc: Terrific leadership team an...\n",
       "1       RT @AmEpilepsySoc: Have you used our interacti...\n",
       "2       Any epilepsy nurses out there using T.E.C to g...\n",
       "3       RT @YellaSunshine: My family suffered a huge l...\n",
       "4       RT @aMundaneSarah: Every day can be a struggle...\n",
       "                              ...                        \n",
       "8618    RT @LotusOak2: #DidYouKnow that of the U.S. ch...\n",
       "8619    RT @LotusOak2: #DidYouKnow that of the U.S. ch...\n",
       "8620    RT @LotusOak2: #DidYouKnow that of the U.S. ch...\n",
       "8621    RT @LotusOak2: #DidYouKnow that of the U.S. ch...\n",
       "8622    #DidYouKnow that of the U.S. children who expe...\n",
       "Name: original_text, Length: 8623, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_cleaned  = tweets_epilepsy.clean_text\n",
    "# data_cleaned\n",
    "data  = tweets_epilepsy.original_text\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Blood test for Down's syndrome hailed http://bbc.in/1BO3eWQ\"               \n",
    "Best_sample = data[5]\n",
    "best_sample2 = data[6]\n",
    "best_sample3 = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @EpiCARE_ERN: Sudden Unexpected Death in EPilepsy (#SUDEP) is a fatal complication of #epilepsy. \\n\\n#SUDEPActionDay2019 is a day to raise‚Ä¶'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @EpiCARE_ERN: #EpiCARE runs case discussion seminars for complex #epilepsy cases of #genetic, #metabolic, #infectious or unknown causes.‚Ä¶\n",
      "<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>\n"
     ]
    }
   ],
   "source": [
    "print(tweets_epilepsy.original_text[5])\n",
    "print(tweets_epilepsy.source[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Any epilepsy nurses out there using T.E.C to gather data on QUOLIE 10, NDDIE or HADS before a patient attends clini‚Ä¶ https://t.co/utnqr6hWZc'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amepilepsysoc terrific leadership team great paners advancing epilepsy awareness treatment thank sharing chi‚Ä¶'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean_tweets(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any epilepsy nurses out there using T.E.C to gather data on QUOLIE 10, NDDIE or HADS before a patient attends clini‚Ä¶ https://t.co/utnqr6hWZc\n",
      " Any epilepsy nurses out there using T.E.C to gather data on QUOLIE 10, NDDIE or HADS before a patient attends clini‚Ä¶\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'epilepsy nurses using tec gather data quolie 10 nddie hads patient attends clini‚Ä¶'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data[2])\n",
    "print(remove_URL(data[2]))\n",
    "text_to_word_list(data[2],unidecode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @AmEpilepsySoc: Have you used our interactive program to plan your #AES2019 experience? Search by date, role, session type, and other he‚Ä¶\n",
      "@amepilepsysoc used interactive program plan #aes2019 experience search date role session type he‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "print(data[1])\n",
    "Sentence1 = text_to_word_list(data[1], unidecode)\n",
    "print(Sentence1)\n",
    "#print(clean_tweets(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h.grissette/.conda/envs/my_env_3.6/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data = data.apply(lambda x: text_to_word_list(x, unidecode))\n",
    "data.to_csv('epilepsy_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h.grissette/.conda/envs/my_env_3.6/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# data_cleaned.to_csv('parkinsons_cleaned_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = data.copy()\n",
    "file_model = file_model[data.str.len()>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_parkinson.clean_text[1]\n",
    "tweets_epilepsy.clean_text = data\n",
    "tweets_epilepsy.to_csv('tweets_epilepsy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:11:32: collecting all words and their counts\n",
      "INFO - 15:11:32: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 15:11:32: collected 25892 word types from a corpus of 67835 words (unigram + bigrams) and 4729 sentences\n",
      "INFO - 15:11:32: using 25892 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 15:11:32: source_vocab length 25892\n",
      "INFO - 15:11:33: Phraser built with 7125 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['amepilepsysoc_have',\n",
       " 'used_interactive',\n",
       " 'program_plan',\n",
       " 'aes2019_experience',\n",
       " 'search_date',\n",
       " 'role_session',\n",
       " 'type_he']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [row for row in file_model]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:12:49: collecting all words and their counts\n",
      "INFO - 15:12:49: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 15:12:49: collected 10534 word types from a corpus of 43095 raw words and 4729 sentences\n",
      "INFO - 15:12:49: Loading a fresh vocabulary\n",
      "INFO - 15:12:49: effective_min_count=3 retains 3167 unique words (30% of original 10534, drops 7367)\n",
      "INFO - 15:12:49: effective_min_count=3 leaves 32957 word corpus (76% of original 43095, drops 10138)\n",
      "INFO - 15:12:49: deleting the raw counts dictionary of 10534 items\n",
      "INFO - 15:12:49: sample=1e-05 downsamples 3167 most-common words\n",
      "INFO - 15:12:49: downsampling leaves estimated 5841 word corpus (17.7% of prior 32957)\n",
      "INFO - 15:12:49: estimated required memory for 3167 words and 300 dimensions: 9184300 bytes\n",
      "INFO - 15:12:49: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:14:31: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseWordEmbeddingsModel.most_similar of <gensim.models.word2vec.Word2Vec object at 0x2aab6ecbd2d0>>\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)\n",
    "print(w2v_model.most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseWordEmbeddingsModel.most_similar of <gensim.models.word2vec.Word2Vec object at 0x2aab6ecbd2d0>>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:16:45: saving Word2Vec object under word2vec.model, separately None\n",
      "INFO - 15:16:45: not storing attribute vectors_norm\n",
      "INFO - 15:16:45: not storing attribute cum_table\n",
      "INFO - 15:16:45: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h.grissette/.conda/envs/my_env_3.6/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "file_export = file_model.copy()\n",
    "file_export.to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 15:48:46: loading Word2Vec object from word2vec.model\n",
      "INFO - 15:48:46: loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "INFO - 15:48:47: setting ignored attribute vectors_norm to None\n",
      "INFO - 15:48:47: loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "INFO - 15:48:47: loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "INFO - 15:48:47: setting ignored attribute cum_table to None\n",
      "INFO - 15:48:47: loaded word2vec.model\n",
      "INFO - 15:48:53: precomputing L2-norms of word weight vectors\n",
      "/home/h.grissette/.conda/envs/my_env_3.6/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "word_vectors = Word2Vec.load(\"word2vec.model\").wv\n",
    "\n",
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)\n",
    "\n",
    "word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)\n",
    "\n",
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]\n",
    "\n",
    "words = pd.DataFrame(word_vectors.vocab.keys())\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])\n",
    "\n",
    "words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value\n",
    "\n",
    "words.head(10)\n",
    "\n",
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>great_paners</td>\n",
       "      <td>-1.005367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>advancing</td>\n",
       "      <td>0.998064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>epilepsy</td>\n",
       "      <td>-0.999886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>awareness</td>\n",
       "      <td>1.001690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>treatment</td>\n",
       "      <td>-1.004009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3162</td>\n",
       "      <td>browse_aicles</td>\n",
       "      <td>1.001180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3163</td>\n",
       "      <td>jcpnwemployers_are</td>\n",
       "      <td>-1.002699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3164</td>\n",
       "      <td>aware_consider</td>\n",
       "      <td>1.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3165</td>\n",
       "      <td>epilepsysociety_produced</td>\n",
       "      <td>1.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3166</td>\n",
       "      <td>guide_sup</td>\n",
       "      <td>-1.005817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3167 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         words  sentiment_coeff\n",
       "0                 great_paners        -1.005367\n",
       "1                    advancing         0.998064\n",
       "2                     epilepsy        -0.999886\n",
       "3                    awareness         1.001690\n",
       "4                    treatment        -1.004009\n",
       "...                        ...              ...\n",
       "3162             browse_aicles         1.001180\n",
       "3163        jcpnwemployers_are        -1.002699\n",
       "3164            aware_consider         1.000008\n",
       "3165  epilepsysociety_produced         1.000963\n",
       "3166                 guide_sup        -1.005817\n",
       "\n",
       "[3167 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dictionary = pd.read_csv('sentiment_dictionary.csv')\n",
    "sentiment_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
